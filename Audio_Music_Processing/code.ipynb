{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "from scipy import signal\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#import cmath\n",
    "#import math\n",
    "\n",
    "import librosa\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "import copy\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sys' (built-in)> : \n",
      "your version  3.7.3 (default, Mar 27 2019, 22:11:17) \n",
      "[GCC 7.3.0] \n",
      "originally used version  3.7.3 \n",
      "\n",
      "<module 'scipy' from '/home/c/anaconda3/lib/python3.7/site-packages/scipy/__init__.py'> : \n",
      "your version  1.4.1 \n",
      "originally used version  1.4.1  \n",
      "\n",
      "<module 'numpy' from '/home/c/anaconda3/lib/python3.7/site-packages/numpy/__init__.py'> : \n",
      "your version  1.19.2 \n",
      "originally used version  1.19.2  \n",
      "\n",
      "<module 'pandas' from '/home/c/anaconda3/lib/python3.7/site-packages/pandas/__init__.py'> : \n",
      "your version  1.1.4 \n",
      "originally used version  1.1.4 \n",
      "\n",
      "<module 'librosa' from '/home/c/.local/lib/python3.7/site-packages/librosa/__init__.py'> : \n",
      "your version  0.9.1 \n",
      "originally used version  0.9.1 \n",
      "\n",
      "<module 'json' from '/home/c/anaconda3/lib/python3.7/json/__init__.py'> : \n",
      "your version  2.0.9 \n",
      "originally used version  2.0.9 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the used versions of imports:\n",
    "import matplotlib, prettytable \n",
    "import_list = [sys, scipy,np,pd,librosa,json] # sys gives us the python version\n",
    "my_versions = ['3.7.3','1.4.1 ','1.19.2 ','1.1.4','0.9.1','2.0.9','']\n",
    "for ele, my_version in zip(import_list, my_versions):\n",
    "    try:\n",
    "        v = ele.__version__\n",
    "        print\n",
    "    except:\n",
    "        try:\n",
    "            v = ele.version\n",
    "            \n",
    "        except:\n",
    "            v = 'cant say version'\n",
    "    print(ele, ': \\nyour version ', v, '\\noriginally used version ', my_version, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(array):\n",
    "    stopping_t = len(array)#-60\n",
    "    if stopping_t < 0:\n",
    "        raise Exception('Inputted array is too small!')\n",
    "        return None\n",
    "    \n",
    "    d1 = np.copy(array)[:stopping_t]\n",
    "    d2 = np.copy(array)[:len(d1)]\n",
    "   \n",
    "    r = [] \n",
    "\n",
    "    for tau in range(0,len(array)):\n",
    "             \n",
    "        summed_multiplication = np.sum(d1*d2)\n",
    "        d1 = d1[1:]\n",
    "        d2 = d2[:-1]\n",
    "\n",
    "        r.append(summed_multiplication)\n",
    "\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorrelation(array1, array2):\n",
    "    \n",
    "    # arrays have same length\n",
    "    \n",
    "    stopping_t = array1.shape[1]#-60\n",
    "    if stopping_t < 0:\n",
    "        raise Exception('Inputted array is too small!')\n",
    "        return None\n",
    "    \n",
    "    d1 = np.copy(array1)[:,:stopping_t]#60:stopping_t\n",
    "    d2 = np.copy(array2)[:,:d1.shape[1]]\n",
    "\n",
    "    first_round = True\n",
    "    for tau in range(0,array1.shape[1]):#60,201):#(0,len(d1)):#(61,201):\n",
    "        \n",
    "\n",
    "        summed_multiplication = np.sum(d1*d2,axis=1).reshape((d1.shape[0],1))\n",
    "        d1 = d1[:,1:]\n",
    "        d2 = d2[:,:-1]\n",
    "        # stopping_t -= 1 # every time tau grows we come closer to the final index of the array. Therefore, \n",
    "        # t has to stop earlier.\n",
    "        if first_round == True:\n",
    "            first_round = False\n",
    "            r = summed_multiplication\n",
    "        else:\n",
    "            r = np.concatenate((r, summed_multiplication),axis=1)\n",
    "\n",
    "        \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def picking_peaks(detection_func_norm_thres, max_ind_value, mean_ind_value, distance, delta):\n",
    "    # the function is a bit more complicated than simply looping over the whole\n",
    "    # onset detection function. With the array 'all_peaks' we want to look only at \n",
    "    # values which are bigger than 0 in the onset detection function. Furthermore,\n",
    "    # we not simply loop over the 'all_peaks function', also here we make jumps when\n",
    "    # a) through a found onset the minimal distance for the next potential onset has\n",
    "    # to be considered and b) when we have within one window for which we search the \n",
    "    # maximal value we have several indices which fulfill the maxima condition. \n",
    "    # Therefore, we would jump to the next maximum with same value when the first was\n",
    "    # not able to be a onset because it could not top the mean of the mean window.\n",
    "    \n",
    "    all_peaks = np.where(detection_func_norm_thres>0)[0]\n",
    "   \n",
    "    \n",
    "    start_searching = None\n",
    "    peak_collector = [] # will be the final list with all onset ind values\n",
    "\n",
    "    # condition 1: has to be a max in a certain window\n",
    "    single_peak_ind = 0\n",
    "    curr_peak_ind = all_peaks[single_peak_ind]\n",
    "    # single_peak_ind: ind in the all_peaks where every value is bigger than 0\n",
    "    # curr_peak_ind: where the bigger-than-zero peak is within the onset detection function\n",
    "\n",
    "    while single_peak_ind < len(all_peaks):\n",
    "\n",
    "        lower_max_ind = (curr_peak_ind-max_ind_value) \n",
    "        if lower_max_ind < 0 or False:\n",
    "            lower_max_ind = 0\n",
    "\n",
    "        if start_searching is not None:\n",
    "            start_searching = None\n",
    "            lower_max_ind = single_peak_ind\n",
    "\n",
    "        upper_max_ind = (curr_peak_ind+max_ind_value) \n",
    "        if upper_max_ind >= len(detection_func_norm_thres) or False:\n",
    "            upper_max_ind = len(detection_func_norm_thres)-1\n",
    "            \n",
    "\n",
    "        max_window = np.array([lower_max_ind,upper_max_ind])\n",
    "        max_windowed_func = detection_func_norm_thres[lower_max_ind:upper_max_ind+1]\n",
    "        found_max_ind = np.argmax(max_windowed_func)\n",
    "        found_max_value = max_windowed_func[found_max_ind]\n",
    "\n",
    "        # find out if there is a second max with the same value:\n",
    "        copied = np.copy(max_windowed_func)\n",
    "        copied[found_max_ind] = 0\n",
    "        found_same_max_ind = np.where(copied == found_max_value)[0]\n",
    "\n",
    "        # condition 2: is the max bigger than the mean of a second surrounding window?\n",
    "        lower_mean_ind = (curr_peak_ind-mean_ind_value) \n",
    "        if lower_mean_ind < 0 or False:\n",
    "            lower_mean_ind = 0\n",
    "\n",
    "        upper_mean_ind = (curr_peak_ind+mean_ind_value) \n",
    "        if upper_mean_ind >= len(detection_func_norm_thres) or False:\n",
    "            upper_mean_ind = len(detection_func_norm_thres)-1\n",
    "\n",
    "        mean_window = np.array([lower_mean_ind,upper_mean_ind])\n",
    "        found_mean_value = np.mean(detection_func_norm_thres[lower_mean_ind:upper_mean_ind+1]) + delta\n",
    "     \n",
    "        if found_max_value >= found_mean_value: # we found an onset\n",
    "            peak_collector.append(curr_peak_ind)\n",
    "\n",
    "            # condition 3: jump to the next point which is fulfilling the distance measurement\n",
    "            if (single_peak_ind+distance) < len(detection_func_norm_thres):\n",
    "                est = curr_peak_ind + distance\n",
    "                try:\n",
    "                    single_peak_ind = np.where(all_peaks>=est)[0][0] # which \n",
    "                    # peak value bigger 0 comes closest to the minimal distance\n",
    "                    curr_peak_ind = all_peaks[single_peak_ind]\n",
    "                except: # single_peak_ind = np.where(all_peaks>=est)[0] = []\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        else: # we did not found an onset\n",
    "            # the next max_window shall start at the index after the maximal value of the \n",
    "            # last used max_window, else the next value would not be able to be bigger than the \n",
    "            # last max\n",
    "\n",
    "            # special case: we have found several maxima with the same value in one max_window.\n",
    "            # Therefore, we jump to them for the next loop. When the mean value lowers, they \n",
    "            # have the chance to be taken as peak:\n",
    "            if found_same_max_ind != []:\n",
    "                #single_peak_ind \n",
    "                curr_peak_ind = lower_max_ind + found_same_max_ind[0]\n",
    "                single_peak_ind = np.where(all_peaks==curr_peak_ind)[0][0]\n",
    "                start_searching = not None # can be anything\n",
    "\n",
    "            else:\n",
    "                # even for values which are not max within the window:\n",
    "                # when the max value vanishes with the moving max_window, they can become the max.\n",
    "                # Second opportunity is that the mean value of the shifted mean_window lowers.\n",
    "                single_peak_ind += 1# upper_max_ind \n",
    "                try:\n",
    "                    curr_peak_ind = all_peaks[single_peak_ind]\n",
    "                except: # we had a look at all peaks bigger than 0\n",
    "                    break\n",
    "    return np.asarray(peak_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_filterbank(sample_rate, hop_size, segment_times, STFT):\n",
    "\n",
    "    # from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/#eqn1\n",
    "    # and https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html\n",
    "\n",
    "    STFT11 = np.copy(STFT)\n",
    "\n",
    "    # 0. finding out the sample rate for one window:\n",
    "    len_window = int(segment_times[1])\n",
    "    in_sec_window = (len_window-1) * hop_size/ sample_rate\n",
    "    sample_rate2 = int(in_sec_window * sample_rate) # that's the number of samples in one window\n",
    "\n",
    "\n",
    "    # 1. lower border of human hearing https://www.audiologyresearch.org/human-hearing-range: \n",
    "    # 20 Hz:\n",
    "    # get borders mel band:\n",
    "    variable = 2959 # values seen in lecture and internet so far: 2959, 1125, 2595\n",
    "    \n",
    "    mel_lowest_f = (variable * np.log10(1 + (20) / 700))\n",
    "    mel_highest_f = (variable * np.log10(1 + (sample_rate2 / 2) / 700))  # Convert Hz to Mel\n",
    "\n",
    "    # 2. creating 40 bins/bands from which 38 are equally distributed between the lowest and\n",
    "    # highest mel value:\n",
    "    number_filters = 20 #40 # normal: 20-40, standard: 26\n",
    "    # with 20 no final_bins with same value and in filterbank no row with set={0.0}\n",
    "\n",
    "    distance_mel = (mel_lowest_f + mel_highest_f)/(number_filters+1)\n",
    "    mel_binDistances = np.array([mel_lowest_f + i * distance_mel for i in range(0, number_filters+2)])\n",
    "\n",
    "    # 3. Converting the the mel_bins into frequencies:\n",
    "    f_binDistances = (700 * (10**(mel_binDistances / variable) - 1)) #700*(np.exp(mel_binDistances/2959)-1)\n",
    "    # because log_10(b)=x leads to 10**x=b\n",
    "\n",
    "    # 4. Missing freq. resolution. Therefore, round the found freq. to nearest DFT bin.\n",
    "    # Need of number of DFT bins and sample_rate:\n",
    "    final_bins = np.floor((STFT.shape[0]+1)*f_binDistances/sample_rate2) # last bin relates\n",
    "    # to mel_highest_f\n",
    "\n",
    "    # 5. Create filterbanks: \n",
    "    # Each filterbank starts at point i, has the peak at i+1 and is zero at i+2.\n",
    "    # formula H_m(k) from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n",
    "    # where k=...-th_filter, f=final_bins. Sentence before describes formula\n",
    "    filterbank = np.zeros((number_filters,int(np.floor(STFT.shape[0] ))))\n",
    "    # rows: number used filterbanks/individual filters, columns: values for certain frequencies\n",
    "\n",
    "    for m in range(1,(number_filters+1)):\n",
    "        left_val = int(final_bins[m-1])\n",
    "        middle_val = int(final_bins[m])\n",
    "        right_val = int(final_bins[m+1])\n",
    "\n",
    "        for k in range(left_val, middle_val): # number of fitlers m between final bins\n",
    "            filterbank[m-1,k] = (k-final_bins[m-1])/(final_bins[m]-final_bins[m-1])\n",
    "        for k in range(middle_val, right_val):\n",
    "            filterbank[m-1,k] = (final_bins[m+1]-k)/(final_bins[m+1]-final_bins[m])\n",
    "\n",
    "    # 6. lecture content L04 slide 54: the magnitude approach:\n",
    "    # Use the filterbank for each DFT magnitude to reduce it in bin size:\n",
    "    \n",
    "    shape_border = int(STFT.shape[1]/2)\n",
    "    S1 = (filterbank @ abs(STFT[:,:shape_border]))\n",
    "    S2 = (filterbank @ abs(STFT[:,:STFT.shape[1]]))\n",
    "    S = np.concatenate((S1,S2),axis=1)\n",
    "    S = 10 * np.log10(S+1) # rows: number of filters, column: like in STFT the \n",
    "\n",
    "\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onset_detection(file):    \n",
    "\n",
    "    sample_rate, data = scipy.io.wavfile.read(file)\n",
    "    \n",
    "    \n",
    "    resampled = data\n",
    "    #resampled = signal.resample(data, num=50000, t=None, axis=0, window=None)\n",
    "    number_overlap = 100\n",
    "    sample_freq, segment_times, STFT = signal.stft(resampled,noverlap=number_overlap) # all have the same size already!\n",
    "    # noverlap: the number of samples two neighbouring DFT have in common / reverse of hop size\n",
    "     \n",
    "    hop_size = int(segment_times[1] - number_overlap)\n",
    "    \n",
    "    STFT = mel_filterbank(sample_rate, hop_size, segment_times, STFT)\n",
    "    # of course its the filtered STFT\n",
    "    \n",
    "    \"\"\" COMPLEX DOMAIN\n",
    "    # Create complex domain:\n",
    "    angle_matrix = np.angle(STFT) # in radians not degrees (algebraic angle here)\n",
    "    exponentialized_matrix1 =np.exp(1j*angle_matrix)\n",
    "\n",
    "    delta_matrix =np.zeros(angle_matrix.shape)\n",
    "    matrix = angle_matrix # phase_matrix\n",
    "    for time in reversed(range(matrix.shape[1])):\n",
    "       # print(np.all(out_matrix[:,-1]==0))\n",
    "\n",
    "        if time==2:\n",
    "            delta_matrix[:, time-1] = (matrix[:,time-1] - matrix[:,time-2]) #- matrix[time-2]\n",
    "\n",
    "        elif time==1:\n",
    "            delta_matrix[:,time-1] = matrix[:,0] # or 0 ???\n",
    "\n",
    "        elif time==0:\n",
    "            break\n",
    "        else:\n",
    "            delta_matrix[:, time-1] = (matrix[:,time-1] - matrix[:,time-2]) - (matrix[:,time-2] - matrix[:,time-3])\n",
    "\n",
    "    exponentialized_matrix2 = np.exp(1j*delta_matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    abs_STFT = np.abs(STFT)\n",
    "    \n",
    "    \"\"\" COMPLEX DOMAIN\n",
    "    STFT_prediction = abs_STFT * exponentialized_matrix1 / exponentialized_matrix2\n",
    "    \n",
    "    substractor_matrix = np.abs(STFT - STFT_prediction)\n",
    "    detection_func = np.sum(substractor_matrix, axis=0) # get for every column/DFT one value\n",
    "    \"\"\"\n",
    "    \n",
    "    spec_diff =  abs_STFT[:,1:] - abs_STFT[:,:-1]\n",
    "    spec_diff[np.where(spec_diff<0)] = 0\n",
    "    detection_func = np.sum(spec_diff, axis=0)\n",
    " \n",
    "    # Postprocess detection function:\n",
    "    \n",
    "    # Normalize detection function:\n",
    "    mean = detection_func.mean()\n",
    "    std = detection_func.std()\n",
    "    detection_func_norm = (detection_func-mean)/std\n",
    "\n",
    "    # Adaptive thresholding:\n",
    "    delta_ = 0 #0.25#0.5 # 3#1 #0\n",
    "    lambda_ = 1\n",
    "    window_size = 25 # shall be odd number!\n",
    "\n",
    "    threshold_array = np.array([])\n",
    "\n",
    "    for curr_ind, curr_pt in enumerate(detection_func_norm):\n",
    "\n",
    "        # getting window taking the index edges into account: \n",
    "        if window_size % 2 == 0: # if it is even\n",
    "            upper = int((window_size)/2) # window is on right sight one step longer\n",
    "            lower = int((window_size)/2)-1\n",
    "        else:\n",
    "            upper = int((window_size-1)/2)\n",
    "            lower = int((window_size-1)/2)\n",
    "\n",
    "        lower_window_ind = max(0, curr_ind-lower)\n",
    "        upper_window_ind = min(len(detection_func_norm)-1, curr_ind+upper)\n",
    "\n",
    "\n",
    "\n",
    "        window = detection_func_norm[lower_window_ind:upper_window_ind+1]\n",
    "        window_size_sorted = np.sort(window)\n",
    "\n",
    "        # getting median value and index:\n",
    "\n",
    "        if window_size % 2 == 0: # if it is even\n",
    "\n",
    "            lower_ind = int((window_size)/2-1)\n",
    "            upper_ind = lower_ind+1\n",
    "\n",
    "            median = (window_size_sorted[lower_ind] + window_size_sorted[upper_ind])/2\n",
    "        else:\n",
    "            median = window_size_sorted[int((window_size-1)/2)]\n",
    "            \n",
    "            \n",
    "\n",
    "        # comparison values:    \n",
    "        threshold_array = np.append(threshold_array, median)\n",
    "    threshold_array = lambda_ * threshold_array + delta_\n",
    "      \n",
    "#     plt.plot([i for i in range(len(detection_func))],detection_func, c='green')\n",
    "#     plt.show()\n",
    "#     plt.plot([i for i in range(len(detection_func_norm))],detection_func_norm, c='red')\n",
    "#     plt.plot([i for i in range(len(threshold_array))],threshold_array,'.', c='blue')\n",
    "#     plt.show()\n",
    "    detection_func_norm_thres = np.copy(detection_func_norm)\n",
    "  \n",
    "    try:\n",
    "        detection_func_norm_thres[np.where(threshold_array>detection_func_norm_thres)[0]] =  min(detection_func_norm_thres)# 0\n",
    "    except:\n",
    "        print('no thres')\n",
    "        pass\n",
    " \n",
    "    # Getting onsets:\n",
    "    \n",
    "    \n",
    "    # Peak picking:\n",
    "    max_window_half_size, mean_window_half_size, distance = 14, 18, 48\n",
    "\n",
    "    indices = picking_peaks(detection_func_norm_thres, max_window_half_size, mean_window_half_size, distance, delta_)\n",
    "  \n",
    "    \n",
    "    predicted_onset_arr_time = list(indices * hop_size / sample_rate) # convertion\n",
    "    \n",
    "    return detection_func_norm_thres, predicted_onset_arr_time, sample_rate, hop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempo_detection(detection_func_norm_thres,splitter,hop_size,sample_rate):\n",
    "    \n",
    "    # Tempo estimation:\n",
    "    \n",
    "    # autocorrelation:\n",
    "    #full_time = librosa.get_duration(data)\n",
    "    \n",
    "    # Compute place in detection autocorrelation function where tempo is 60 and 200 bpm:\n",
    "    place_60 = 60 * sample_rate / (splitter* 60 * hop_size)\n",
    "    place_200 = 60 * sample_rate / (splitter*200 * hop_size)\n",
    "    lower_ind = int(place_200)+1\n",
    "    upper_ind = int(place_60)\n",
    "    \n",
    "    # Splitting the detection function into windows such that we can also consider different tempi in only one\n",
    "    # piece:\n",
    "    window_len = int(len(detection_func_norm_thres)/splitter)\n",
    "    window_ind = np.array([i*window_len-1 for i in range(1,splitter+1)])\n",
    "    last_ind = 0\n",
    "    tempo_list = []\n",
    "    indicating_list = []\n",
    "    start = True\n",
    "    inder = []\n",
    "    \n",
    "    '''full autocorrelation without windows:\n",
    "    place_60 = 60 * sample_rate / ( 60 * hop_size)\n",
    "    place_200 = 60 * sample_rate / (200 * hop_size)\n",
    "    lower_ind = int(place_200)+1\n",
    "    upper_ind = int(place_60)\n",
    "    r = autocorrelation(detection_func_norm_thres)    \n",
    "    r2 = r[lower_ind:upper_ind+1]\n",
    "    r2 = r2[1:]\n",
    "    tempo_ind = np.argmax(r2)#+1\n",
    "    tempo_full_ind = lower_ind + tempo_ind\n",
    "\n",
    "    tempo_found =   60 * sample_rate / ((tempo_full_ind+1) * hop_size)\n",
    "    \n",
    "    r2.remove(r2[tempo_ind])\n",
    "    tempo_ind2 = np.argmax(r2)\n",
    "    tempo_full_ind2 = lower_ind + tempo_ind2\n",
    "    tempo_found2 =   60 * sample_rate / ( (tempo_full_ind2+1) * hop_size)\n",
    "    return [tempo_found, tempo_found2],[tempo_found,tempo_found2], 0, detection_func_norm_thres, 0\n",
    "    #return true_tempi, tempi_window_sec, start_window_sec, detection_window_,start_window_ind\n",
    "    '''\n",
    "    # autocorrlation windowed:\n",
    "    \n",
    "    tempi_window_sec, start_window_sec, start_window_ind ,detection_window_ = [], [],[], None\n",
    "\n",
    "\n",
    "    for ind in window_ind:\n",
    "        detection_window = detection_func_norm_thres[last_ind:ind+1]\n",
    "        lower_border_sec =  last_ind * hop_size / sample_rate#60 * sample_rate / (splitter* last_ind * hop_size)\n",
    "\n",
    "        start_window_sec.append(lower_border_sec)  \n",
    "       # start_window_sec.append(lower_border_sec) \n",
    "        \n",
    "        start_window_ind.append(last_ind)  \n",
    "       # start_window_ind.append(last_ind) \n",
    "        \n",
    "        ind_saved = last_ind\n",
    "        last_ind = ind + 1\n",
    "        r_windowed = autocorrelation(detection_window)#detection_func_norm_thres) # r gives us the raw autocorrelation of d[t]\n",
    "        r_windowed_cutted = r_windowed[lower_ind:upper_ind+1]\n",
    "        tempo_ind = np.argmax(r_windowed_cutted)\n",
    "        \n",
    "        r_windowed_cutted[tempo_ind] = -1\n",
    "        tempo_ind2 = np.argmax(r_windowed_cutted) # get second highest value\n",
    "\n",
    "        tempo_full_ind = lower_ind + tempo_ind \n",
    "        tempo_found =  60 * sample_rate / (splitter* tempo_full_ind * hop_size)\n",
    "        tempo_list.append(tempo_found)\n",
    "        indicating_list.append(r_windowed_cutted[tempo_ind2])\n",
    "\n",
    "        tempo_full_ind2 = lower_ind + tempo_ind2\n",
    "        tempo_found2 = 60 * sample_rate / (splitter* tempo_full_ind2 * hop_size)\n",
    "        tempo_list.append(tempo_found2)\n",
    "        if tempo_found2 > tempo_found:\n",
    "            insert = [tempo_found,tempo_found2]\n",
    "        else:\n",
    "            insert = [tempo_found2,tempo_found]\n",
    "        tempo_array_part = np.array(insert).reshape((1,2))\n",
    "        if start == True:\n",
    "            start = False\n",
    "            tempo_array = tempo_array_part\n",
    "        else:\n",
    "            tempo_array = np.concatenate([tempo_array, tempo_array_part],axis=0)\n",
    "\n",
    "        tempo_window_sec = tempo_full_ind * hop_size / sample_rate #60 * sample_rate / (splitter* tempo_full_ind * hop_size) # assuming window starts at t=0\n",
    "        tempo_window_sec2 = tempo_full_ind2 * hop_size / sample_rate#60 * sample_rate / (splitter* tempo_full_ind2 * hop_size)\n",
    "        tempi_window_sec.append(tempo_window_sec)\n",
    "        #tempi_window_sec.append(tempo_window_sec2)\n",
    "\n",
    "\n",
    "\n",
    "        if detection_window_ is None:\n",
    "            detection_window_ = detection_window.reshape(1,len(detection_window))\n",
    "        else:\n",
    "            detection_window_ = np.concatenate((detection_window_,detection_window.reshape(1,len(detection_window))),axis=0)\n",
    "\n",
    "\n",
    "        inder.append(tempo_full_ind+ind_saved) # to not only look at window of detection function but also \n",
    "        # to full detection function. where is the tempo index in there?\n",
    "        inder.append(tempo_full_ind2+ind_saved)\n",
    "\n",
    "\n",
    "    # Taking median:\n",
    "    tempo_array = np.sort(tempo_array, axis=0)\n",
    "    tempo_array2 = np.copy(tempo_array)\n",
    "    if tempo_array.shape[0] %2 == 0: # even number of rows\n",
    "        upper_middle = int(tempo_array.shape[0] /2 )# upper one because we want index\n",
    "        lower_middle = upper_middle -1 \n",
    "        median_value = (tempo_array[upper_middle,:]+tempo_array[lower_middle,:])/2\n",
    "\n",
    "    else:\n",
    "        middle = int(tempo_array.shape[1] /2)\n",
    "        median_value = tempo_array[middle,:]\n",
    "\n",
    "    #dd[title]['tempo'] = median_value.tolist()\n",
    "    \n",
    "    tempo1 = max(tempo_list)\n",
    "\n",
    "    tempo2 = min(tempo_list)\n",
    "\n",
    "    where_max = tempo_list.index(tempo1)\n",
    "    where_min = tempo_list.index(tempo2)\n",
    "    detection_func_tempo1 = inder[where_max]\n",
    "    detection_func_tempo2 = inder[where_min]\n",
    "    \"\"\"\n",
    "    # take two tempi with highest indications:\n",
    "    tempo_value1 = tempo_list[np.argmax(indicating_list)]\n",
    "    tempo_list.remove(tempo_value1)\n",
    "    indicating_list.remove(max(indicating_list))\n",
    "    tempo_value2 = tempo_list[np.argmax(indicating_list)]\n",
    "\n",
    "    if tempo_value1 > tempo_value2:\n",
    "        sol = [tempo_value2, tempo_value1]\n",
    "    else:\n",
    "        sol = [tempo_value1, tempo_value2] \n",
    "\n",
    "    meaning_sol = np.mean(tempo_array2,axis=0)     \n",
    "\n",
    "    if tempo_found > tempo_f\n",
    "    tempo_found]\n",
    "    else:\n",
    "        sol = [tempo_found, tempo_found2] \n",
    "    \"\"\"  \n",
    "    true_tempi = [tempo1]#[tempo2, tempo1]\n",
    "\n",
    "    return true_tempi, tempi_window_sec, start_window_sec, detection_window_,start_window_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_detection(start_window_ind, start_window_sec, tempo_list_sec, detection_func_norm_thres_wind, hop_size, splitter, sample_rate, predicted_onset_arr_time):\n",
    "    \n",
    "    \n",
    "    onset_func_sec = detection_func_norm_thres_wind #* hop_size / sample_rate #60 * sample_rate / (splitter* detection_func_norm_thres_wind * hop_size)\n",
    "    indd = np.array([i for i in range(onset_func_sec.shape[1])])\n",
    "    onset_function_x_axis = indd #list(indd * hop_size / sample_rate) \n",
    "  \n",
    "    # 1. compute for each windowed tempo a sinus/cosine/pulse train \n",
    "    # by the shape=(number tempi, single function values):\n",
    "    \n",
    "    onset_function_x_axis_rep = np.array(start_window_ind).reshape((len(start_window_ind),1)) + np.vstack(([onset_function_x_axis for i in range(len(tempo_list_sec))]))\n",
    "                                            \n",
    "    cos2 = lambda x: abs(np.cos(2*np.pi*np.asarray(tempo_list_sec).reshape((len(tempo_list_sec),1)) * np.array(x)))\n",
    "   \n",
    "    # We want that the cos has its first peak at the period=1/tempo, therefore we use cos(B*x)\n",
    "    # with the formula period = 2 pi/ abs(B) such that abs(B) = 2 pi tempo.\n",
    "    cos2_sampled = cos2(onset_function_x_axis_rep)\n",
    "\n",
    "    # 2. Cross-correlation between computed function from 1. and detection window recomputed in seconds\n",
    "    # (each starting at second=0):\n",
    "    onset_sec_lifted = onset_func_sec  + abs(min(onset_func_sec.flatten() )) # lowest y-value of function shall be 0\n",
    "    r_cross2 = crosscorrelation(cos2_sampled,onset_sec_lifted)\n",
    "\n",
    "    f1 = np.asarray([[1, 0, 1]])\n",
    "    valley_inds = np.where(r_cross2 < scipy.ndimage.minimum_filter(r_cross2, footprint=f1, mode='reflect', cval=-np.inf))\n",
    "    # fancier method than nested for loop. Compare for each line where one value is smaller than both of its neighbours\n",
    "    # compare each value with horizontal neighbours. When it is smaller than both we have a valley. We need that\n",
    "    # because we want to ignore the first hill where cross-correlation is of course very high \n",
    "    # (correlating two full arrays with each other). \n",
    "    \n",
    "    r_cross2_copy = np.copy(r_cross2)\n",
    "    if len(valley_inds[0]) > 0:\n",
    "        \n",
    "        valley_inds_rows, ind_val = np.unique(valley_inds[0], return_index=True)\n",
    "        valley_inds_col = valley_inds[1][ind_val]\n",
    "\n",
    "        for row_ind in range(r_cross2.shape[0]):\n",
    "            curr_col = valley_inds_col[row_ind]\n",
    "            r_cross2_copy[row_ind][:curr_col+1] = -9999\n",
    "    else:\n",
    "        r_cross2_copy[:,0] = -9999\n",
    "\n",
    "    \n",
    "    beat_indices_part = np.argmax(r_cross2_copy,axis=1) # getting for each line the first maximal value, here our first beat is located\n",
    "    # getting beat for each time window beat estimation\n",
    "    # 1D array!\n",
    "    # in ind of seconds function!\n",
    "    # we have small cross-correlation window in which we search for our max with ignoring the first hill. \n",
    "    # Here we get the final ind of first beat for EACH cross-correlations (which number is the number of estimated tempi)\n",
    "\n",
    "    \n",
    "    beat_indices =  beat_indices_part #np.asarray(start_window_ind) + beat_indices_part\n",
    "    beat_ind = beat_indices\n",
    "    \n",
    "    # 3. Compute the following beat values in seconds within the single onset detection function windows:\n",
    "    next_beat_indices = beat_indices # 1D array!\n",
    "    beat_collector_sec = [] # 4. collect all estimated beats in seconds\n",
    "    number_beats = len(beat_indices)\n",
    "    #onset_function_x_axis_rep = np.vstack(([(onset_function_x_axis for i in range(number_beats)]))\n",
    "    \n",
    "    while np.any(next_beat_indices) < onset_func_sec.shape[1]: #*splitter:\n",
    "        #next_beat_sec = onset_function_x_axis_rep[valley_inds_rows, next_beat_indices]\n",
    "        \n",
    "        forbidden_ind = set(np.where(next_beat_indices>=onset_function_x_axis_rep.shape[1])[0])\n",
    "       \n",
    "        if len(forbidden_ind) > 0:\n",
    "            \n",
    "            if len(forbidden_ind) == len(next_beat_indices):\n",
    "                break\n",
    "            \n",
    "            full_ind_arr = set([i for i in range(len(next_beat_indices))])\n",
    "            \n",
    "            allowed_ind = np.asarray(list(full_ind_arr - forbidden_ind))\n",
    "            \n",
    "            \n",
    "            next_beat_indices = next_beat_indices[allowed_ind]\n",
    "            beat_ind = beat_ind[allowed_ind]\n",
    "            start_window_ind = np.asarray(start_window_ind)[allowed_ind]\n",
    "            number_beats -= len(forbidden_ind )\n",
    "        \n",
    "        next_beat_sec = (start_window_ind + next_beat_indices) * hop_size / sample_rate \n",
    "        \n",
    "        if len(next_beat_sec) == 0:\n",
    "            break\n",
    "        \n",
    "        #closest_onsets_ind = np.where(((np.asarray(predicted_onset_arr_time)>=next_beat_sec) & (np.asarray(predicted_onset_arr_time)<=next_beat_sec+0.05)) | ((np.asarray(predicted_onset_arr_time)<=next_beat_sec) & (np.asarray(predicted_onset_arr_time)>=next_beat_sec-0.05)))\n",
    "        for i in range(number_beats):\n",
    "            closest_onsets_ind = np.where(((np.asarray(predicted_onset_arr_time)>=next_beat_sec[i]) & (np.asarray(predicted_onset_arr_time)<=next_beat_sec[i]+0.05)) | ((np.asarray(predicted_onset_arr_time)<=next_beat_sec[i]) & (np.asarray(predicted_onset_arr_time)>=next_beat_sec[i]-0.05)))\n",
    "            \n",
    "            # find the onset which is closest to our beat location estimation:\n",
    "            try:\n",
    "                closest_onset_ind_ind = np.argmin(abs(np.asarray(predicted_onset_arr_time)[closest_onsets_ind]-next_beat_sec[i]))\n",
    "                closest_onset_ind = closest_onsets_ind[0][closest_onset_ind_ind]\n",
    "                closest_onset_sec = predicted_onset_arr_time[closest_onset_ind]\n",
    "            except: # the closest_onsets_ind is empty. We have no onsets that are close enough to our beat estimation point. \n",
    "                # Therefore, we take the beat estimation point directly as next beat assumption.\n",
    "                closest_onset_sec = next_beat_sec[i]\n",
    "                        \n",
    "            beat_collector_sec.append(closest_onset_sec )\n",
    "        \n",
    "        next_beat_indices += beat_ind\n",
    "            \n",
    "        \n",
    "    return list(np.sort(beat_collector_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"TestData/test/\"\n",
    "#path = \"TrainData/train/\"\n",
    "\n",
    "filenames = glob.glob(path + \"*.wav\") # read all the files with extension .wav\n",
    "label_filenames = glob.glob(path + '*.onsets.gt')\n",
    "tempi_filenames = glob.glob(path + '*.tempo.gt')\n",
    "beats_filenames = glob.glob(path + '*.beats.gt')\n",
    "\n",
    "if label_filenames == []:\n",
    "    label_filenames = np.ones(len(filenames))\n",
    "if tempi_filenames == []:\n",
    "    tempi_filenames = np.ones(len(filenames))\n",
    "if beats_filenames == []:\n",
    "    beats_filenames = np.ones(len(filenames))\n",
    "\n",
    "  \n",
    "# Create an output dictionary:      \n",
    "titles = [(ele.split('/')[-1]).replace('.wav','') for ele in filenames]\n",
    "dd = defaultdict(dict)\n",
    "for title in titles:\n",
    "    dd[title]['onsets'] = None\n",
    "    dd[title]['beats'] = None\n",
    "    dd[title]['tempo'] = None\n",
    "\n",
    "    \n",
    "dd_true = copy.deepcopy(dd)\n",
    "\n",
    "# for loop to iterate all csv files\n",
    "STFT_torch_list = [] # fill it with all single STFTs\n",
    "label_torch_list = [] # fill it with all sample labels\n",
    "\n",
    "for file, label, tempi, beats in zip(filenames, label_filenames, tempi_filenames, beats_filenames):\n",
    "    sample_rate, data = scipy.io.wavfile.read(file)\n",
    "    \n",
    "\n",
    "    detection_func_norm_thres, predicted_onset_arr_time, sample_rate, hop_size = onset_detection(file)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        onset_file_content = pd.read_csv(label,header=None)\n",
    "        true_onset_arr_time = list(np.array(onset_file_content).reshape((len(onset_file_content ,))))# from shape [x,] we come to shape [1,x]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "    \n",
    "    # Getting title of file:\n",
    "    title = (file.split('/')[-1]).replace('.wav','') \n",
    "    \n",
    "    # Typing result into dictionary:\n",
    "    dd[title]['onsets'] = list(predicted_onset_arr_time)\n",
    "    \n",
    "    try:\n",
    "        dd_true[title]['onsets'] = true_onset_arr_time \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "    splitter = 7 #int(full_time/6)+1 # each window shall have a size of circa 5 seconds\n",
    "    found_tempi, tempi_window_sec, start_window_sec, detection_window_,start_window_ind = tempo_detection(detection_func_norm_thres=detection_func_norm_thres,splitter=splitter,hop_size=hop_size,sample_rate=sample_rate)\n",
    "    beats_sec = beat_detection(start_window_ind=start_window_ind, start_window_sec=start_window_sec,tempo_list_sec=tempi_window_sec,detection_func_norm_thres_wind = detection_window_, hop_size=hop_size, splitter=splitter, sample_rate=sample_rate, predicted_onset_arr_time=predicted_onset_arr_time)\n",
    "    \n",
    "    dd[title]['tempo'] = found_tempi#sol#[tempo_found]# [tempo_value1]#[tempo1]# meaning_sol #sol #tempo_list#[tempo2, tempo1]\n",
    "    dd[title]['beats'] = beats_sec\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        tempo_file_content1 = pd.read_csv(tempi,header=None,sep='\\t',dtype='a')#[0]\n",
    "        tempo_file_content2 = pd.read_csv(tempi,header=None,sep='\\t',dtype='a')#[1]\n",
    "        true_tempi = [float(tempo_file_content1.values[0][0]),float(tempo_file_content2.values[0][1]),float(tempo_file_content2.values[0][2])]#.reshape((len(tempo_file_content) ,)))\n",
    "        dd_true[title]['tempo'] = true_tempi\n",
    "    except:\n",
    "        dd_true[title]['tempo'] = None\n",
    "    \n",
    "    try:\n",
    "        true_beats = pd.read_csv(beats,header=None,sep='\\t')\n",
    "        true_beats = np.array(true_beats).flatten()\n",
    "        dd_true[title]['beats'] = list(np.sort(true_beats))\n",
    "    except:\n",
    "        dd_true[title]['beats'] = None\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "dumped = json.dumps(dd)\n",
    "with open('final_predictions.json', 'w') as f:\n",
    "    f.write(dumped + '\\n')\n",
    "\n",
    "dumped_true = json.dumps(dd_true)\n",
    "with open('groundtruth.json','w') as g:\n",
    "    g.write(dumped_true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
